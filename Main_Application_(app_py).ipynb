{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbr/sessiion15-code-refactor/blob/main/Main_Application_(app_py).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py (Modified)\n",
        "import os\n",
        "import chainlit as cl\n",
        "from langchain.schema.runnable.config import RunnableConfig\n",
        "import asyncio\n",
        "\n",
        "# Import configurations and services\n",
        "from config_models import load_settings, AppSettings\n",
        "from services import DocumentService, LanguageModelService, RAGChainService\n",
        "\n",
        "# ---- GLOBAL APP INITIALIZATION ---- #\n",
        "# Load application settings once when the application starts\n",
        "# This will also load .env variables and perform basic validation.\n",
        "try:\n",
        "    APP_SETTINGS: AppSettings = load_settings()\n",
        "except ValueError as e:\n",
        "    print(f\"FATAL: Configuration error: {e}\")\n",
        "    # In a real app, you might exit or have a fallback mechanism\n",
        "    # For Chainlit, it might be hard to stop server startup here directly,\n",
        "    # errors will be logged, and on_chat_start might fail.\n",
        "    APP_SETTINGS = None # Indicate failure\n",
        "    # raise # Optionally re-raise to stop if possible\n",
        "\n",
        "# Initialize services. These can be singletons if their state is managed appropriately.\n",
        "# For Chainlit, services are typically initialized once and then accessed.\n",
        "if APP_SETTINGS:\n",
        "    DOCUMENT_SERVICE = DocumentService(\n",
        "        retriever_config=APP_SETTINGS.retriever_config,\n",
        "        embed_endpoint_url=APP_SETTINGS.hf_embed_endpoint_url,\n",
        "        api_token=APP_SETTINGS.hf_api_token\n",
        "    )\n",
        "    LLM_SERVICE = LanguageModelService(\n",
        "        llm_config=APP_SETTINGS.llm_config,\n",
        "        endpoint_url=APP_SETTINGS.hf_llm_endpoint_url,\n",
        "        api_token=APP_SETTINGS.hf_api_token\n",
        "    )\n",
        "    RAG_CHAIN_SERVICE = RAGChainService(\n",
        "        prompt_config=APP_SETTINGS.prompt_config,\n",
        "        llm_service=LLM_SERVICE\n",
        "    )\n",
        "else: # Handle case where settings failed to load\n",
        "    DOCUMENT_SERVICE = None\n",
        "    LLM_SERVICE = None\n",
        "    RAG_CHAIN_SERVICE = None\n",
        "    print(\"WARNING: Services not initialized due to configuration errors.\")\n",
        "\n",
        "\n",
        "# Global flag and lock to ensure vector store initialization happens once.\n",
        "_vectorstore_initialized_event = asyncio.Event()\n",
        "_initialization_lock = asyncio.Lock()\n",
        "\n",
        "async def ensure_services_initialized():\n",
        "    \"\"\"\n",
        "    Ensures that the DocumentService (and its vector store) is initialized.\n",
        "    This is crucial before the first chat starts if the retriever is needed.\n",
        "    \"\"\"\n",
        "    async with _initialization_lock:\n",
        "        if not _vectorstore_initialized_event.is_set():\n",
        "            if not DOCUMENT_SERVICE:\n",
        "                print(\"Error: DocumentService not available for initialization.\")\n",
        "                # This indicates a severe config problem from startup.\n",
        "                # We might want to signal this to the user in on_chat_start.\n",
        "                return False # Indicate failure\n",
        "\n",
        "            print(\"ensure_services_initialized: Starting DocumentService initialization (vector store)...\")\n",
        "            try:\n",
        "                # force_reindex can be set to True to always re-index on startup,\n",
        "                # or get this from AppSettings if you want it configurable.\n",
        "                await DOCUMENT_SERVICE.initialize_retriever(force_reindex=False)\n",
        "                _vectorstore_initialized_event.set() # Signal that initialization is complete\n",
        "                print(\"ensure_services_initialized: DocumentService initialization complete.\")\n",
        "                return True # Indicate success\n",
        "            except FileNotFoundError as fnf_error:\n",
        "                print(f\"Error during DocumentService initialization: {fnf_error}\")\n",
        "                # This error is critical if documents are expected.\n",
        "                # The app might not be usable.\n",
        "                # Consider how to communicate this to the user in on_chat_start.\n",
        "                return False # Indicate failure\n",
        "            except Exception as e:\n",
        "                print(f\"Error during DocumentService initialization: {e}\")\n",
        "                # Other unexpected errors.\n",
        "                return False # Indicate failure\n",
        "    return True # Already initialized or successfully initialized now\n",
        "\n",
        "\n",
        "# ---- CHAINLIT HOOKS ---- #\n",
        "\n",
        "@cl.author_rename\n",
        "def rename(original_author: str):\n",
        "    \"\"\"Renames the 'Assistant' author using the name from settings.\"\"\"\n",
        "    if APP_SETTINGS and original_author == \"Assistant\":\n",
        "        return APP_SETTINGS.assistant_name\n",
        "    return original_author\n",
        "\n",
        "@cl.on_chat_start\n",
        "async def start_chat():\n",
        "    \"\"\"Called at the start of every user session.\"\"\"\n",
        "    if not APP_SETTINGS or not DOCUMENT_SERVICE or not RAG_CHAIN_SERVICE:\n",
        "        await cl.Message(content=\"Critical error: Application settings or services failed to load. The chatbot may not function correctly. Please check server logs.\").send()\n",
        "        cl.user_session.set(\"lcel_rag_chain\", None)\n",
        "        return\n",
        "\n",
        "    # Ensure services, especially the vector store, are initialized.\n",
        "    # This will block here until initialization is done or fails.\n",
        "    init_success = await ensure_services_initialized()\n",
        "\n",
        "    if not init_success or not _vectorstore_initialized_event.is_set():\n",
        "        await cl.Message(content=\"Error: Could not initialize the document retrieval system. The chatbot may not be able to answer questions based on documents. Please check server logs.\").send()\n",
        "        cl.user_session.set(\"lcel_rag_chain\", None)\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        retriever = DOCUMENT_SERVICE.get_retriever()\n",
        "    except ValueError as e: # If get_retriever is called before init somehow (should be caught by event)\n",
        "        await cl.Message(content=f\"Error: Could not get retriever. {e} Please check server logs.\").send()\n",
        "        cl.user_session.set(\"lcel_rag_chain\", None)\n",
        "        return\n",
        "\n",
        "    # Build the RAG chain using the initialized retriever\n",
        "    lcel_rag_chain = RAG_CHAIN_SERVICE.build_chain(retriever)\n",
        "    cl.user_session.set(\"lcel_rag_chain\", lcel_rag_chain)\n",
        "\n",
        "    await cl.Message(\n",
        "        content=f\"Hello! I am the {APP_SETTINGS.assistant_name}. How can I help you with Paul Graham's essays today?\"\n",
        "    ).send()\n",
        "\n",
        "\n",
        "@cl.on_message\n",
        "async def main_message_handler(message: cl.Message): # Renamed to avoid conflict with original main() if it existed\n",
        "    \"\"\"Called every time a message is received from a session.\"\"\"\n",
        "    lcel_rag_chain = cl.user_session.get(\"lcel_rag_chain\")\n",
        "\n",
        "    if not lcel_rag_chain:\n",
        "        await cl.Message(content=\"The RAG chain is not available, possibly due to an initialization error. Please try restarting the chat or contact support.\").send()\n",
        "        return\n",
        "\n",
        "    # Prepare the input for the chain. It expects a dictionary.\n",
        "    # If your chain's RunnablePassthrough() is for the whole input, pass message.content directly.\n",
        "    # If it's for a specific key like \"query\", then: chain_input = {\"query\": message.content}\n",
        "    chain_input = {\"query\": message.content} # Matching the RAGChainService setup\n",
        "\n",
        "    msg_ui = cl.Message(content=\"\") # Initialize an empty message for streaming\n",
        "    await msg_ui.send()\n",
        "\n",
        "    full_response = \"\"\n",
        "    async for chunk in lcel_rag_chain.astream(\n",
        "        chain_input, # Pass the structured input\n",
        "        config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n",
        "    ):\n",
        "        await msg_ui.stream_token(chunk)\n",
        "        full_response += chunk\n",
        "\n",
        "    # msg_ui.content = full_response # Update with full content if needed, stream_token should handle UI\n",
        "    await msg_ui.update() # Ensure the final message state is saved/updated\n",
        "\n",
        "# Optional: A way to run the initialization outside of Chainlit's direct flow if needed,\n",
        "# for example, in a __main__ block or a separate setup script.\n",
        "# However, for Chainlit, ensure_services_initialized called from on_chat_start\n",
        "# is a common pattern for async setup that needs to complete before handling requests.\n",
        "\n",
        "# To run this app:\n",
        "# 1. Save the files as config_models.py, services.py, app.py in the same directory.\n",
        "# 2. Create a .env file in that directory with your HF_TOKEN, HF_LLM_ENDPOINT, HF_EMBED_ENDPOINT.\n",
        "#    Example .env:\n",
        "#    HF_TOKEN=\"your_huggingface_token\"\n",
        "#    HF_LLM_ENDPOINT=\"your_llm_inference_endpoint_url\"\n",
        "#    HF_EMBED_ENDPOINT=\"your_embedding_inference_endpoint_url\"\n",
        "# 3. Create a subdirectory named 'data' and place 'paul_graham_essays.txt' in it (or let the dummy file be created).\n",
        "# 4. Install necessary packages: pip install chainlit langchain pydantic python-dotenv langchain-huggingface faiss-cpu tqdm sentence-transformers\n",
        "#    (sentence-transformers might not be strictly needed if only using HF_EMBED_ENDPOINT, but good for general Langchain use)\n",
        "# 5. Run: chainlit run app.py -w"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "yPILgr1th6Zv"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}