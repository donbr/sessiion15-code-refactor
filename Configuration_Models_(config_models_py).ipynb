{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbr/sessiion15-code-refactor/blob/main/Configuration_Models_(config_models_py).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# config_models.py\n",
        "import os\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "class EmbeddingConfig(BaseModel):\n",
        "    \"\"\"Configuration for HuggingFace Embeddings.\"\"\"\n",
        "    # If using a specific model from HuggingFace Hub directly (e.g., for sentence-transformers)\n",
        "    # model_name: str = Field(default=\"sentence-transformers/all-MiniLM-L6-v2\", description=\"Default embedding model name\")\n",
        "    # If using an HF Inference Endpoint for embeddings\n",
        "    endpoint_url_env_var: str = Field(default=\"HF_EMBED_ENDPOINT\", description=\"Environment variable for the embedding endpoint URL.\")\n",
        "    # Common parameters for HuggingFaceEndpointEmbeddings\n",
        "    task: str = Field(default=\"feature-extraction\", description=\"Task for the embedding endpoint.\")\n",
        "    # You can add other HuggingFaceEndpointEmbeddings parameters if needed, e.g., model_kwargs\n",
        "\n",
        "class TextLoaderConfig(BaseModel):\n",
        "    \"\"\"Configuration for loading text documents.\"\"\"\n",
        "    document_path: str = Field(description=\"Path to the text file containing documents.\")\n",
        "    # encoding: str = Field(default=\"utf-8\") # Example: if you need to specify encoding\n",
        "\n",
        "class TextSplitterConfig(BaseModel):\n",
        "    \"\"\"Configuration for text splitting.\"\"\"\n",
        "    chunk_size: int = Field(default=1000, description=\"Size of text chunks.\")\n",
        "    chunk_overlap: int = Field(default=200, description=\"Overlap between text chunks.\")\n",
        "    # add_start_index: bool = Field(default=True) # Example of another parameter for RecursiveCharacterTextSplitter\n",
        "\n",
        "class VectorStoreConfig(BaseModel):\n",
        "    \"\"\"Configuration for the vector store.\"\"\"\n",
        "    persist_directory: Optional[str] = Field(default=\"faiss_index_store\", description=\"Directory to save/load the FAISS index. If None, in-memory only.\")\n",
        "    # You might add specific FAISS parameters here if needed\n",
        "\n",
        "class RetrieverConfig(BaseModel):\n",
        "    \"\"\"Configuration for the retrieval component.\"\"\"\n",
        "    loader_config: TextLoaderConfig\n",
        "    splitter_config: TextSplitterConfig\n",
        "    embedding_config: EmbeddingConfig\n",
        "    vector_store_config: VectorStoreConfig\n",
        "    indexing_batch_size: int = Field(default=32, description=\"Batch size for indexing documents into FAISS.\")\n",
        "    # top_k_retrieval: int = Field(default=4, description=\"Number of documents to retrieve by the retriever.\") # Example\n",
        "\n",
        "class PromptConfig(BaseModel):\n",
        "    \"\"\"Configuration for RAG prompts.\"\"\"\n",
        "    template_string: str = Field(\n",
        "        default=(\n",
        "            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context \"\n",
        "            \"to answer the question. If you don't know the answer, just say that you don't know. \"\n",
        "            \"Use three sentences maximum and keep the answer concise.\\n\"\n",
        "            \"Question: {query}\\n\"\n",
        "            \"Context: {context}\\n\"\n",
        "            \"Answer:\"\n",
        "        ),\n",
        "        description=\"The template string for the RAG prompt.\"\n",
        "    )\n",
        "    input_variables: list[str] = Field(default_factory=lambda: [\"context\", \"query\"])\n",
        "\n",
        "class LLMConfig(BaseModel):\n",
        "    \"\"\"Configuration for the HuggingFace LLM Endpoint.\"\"\"\n",
        "    endpoint_url_env_var: str = Field(default=\"HF_LLM_ENDPOINT\", description=\"Environment variable for the LLM endpoint URL.\")\n",
        "    # Common parameters for HuggingFaceEndpoint\n",
        "    task: str = Field(default=\"text-generation\", description=\"Task for the LLM endpoint.\")\n",
        "    max_new_tokens: int = Field(default=512, description=\"Maximum new tokens to generate.\")\n",
        "    top_k: Optional[int] = Field(default=10, description=\"Top-k sampling parameter.\")\n",
        "    top_p: Optional[float] = Field(default=0.95, description=\"Top-p (nucleus) sampling parameter.\")\n",
        "    typical_p: Optional[float] = Field(default=0.95, description=\"Typical-p sampling parameter.\")\n",
        "    temperature: float = Field(default=0.7, description=\"Sampling temperature.\")\n",
        "    repetition_penalty: Optional[float] = Field(default=1.03, description=\"Repetition penalty.\")\n",
        "    # You can add other HuggingFaceEndpoint parameters as needed\n",
        "\n",
        "class AppSettings(BaseModel):\n",
        "    \"\"\"Master configuration for the application, loading from environment variables.\"\"\"\n",
        "    hf_token_env_var: str = Field(default=\"HF_TOKEN\", description=\"Environment variable for the HuggingFace API Token.\")\n",
        "\n",
        "    # Nested configurations\n",
        "    retriever_config: RetrieverConfig\n",
        "    prompt_config: PromptConfig = PromptConfig() # Use default prompt config\n",
        "    llm_config: LLMConfig = LLMConfig() # Use default LLM config\n",
        "\n",
        "    # Chainlit specific settings (optional)\n",
        "    assistant_name: str = Field(default=\"Paul Graham Essay Bot\")\n",
        "\n",
        "    # Actual loaded values from env vars\n",
        "    hf_llm_endpoint_url: Optional[str] = None\n",
        "    hf_embed_endpoint_url: Optional[str] = None\n",
        "    hf_api_token: Optional[str] = None\n",
        "\n",
        "    class Config:\n",
        "        env_file = \".env\" # Specify .env file for Pydantic to load from (optional, can also use load_dotenv explicitly)\n",
        "        extra = \"ignore\" # Ignore extra fields from .env\n",
        "\n",
        "# Function to load configuration and environment variables\n",
        "def load_settings() -> AppSettings:\n",
        "    \"\"\"Loads settings, ensuring environment variables are accessible.\"\"\"\n",
        "    load_dotenv() # Explicitly load .env, useful for broader compatibility\n",
        "\n",
        "    # Example: Define specific paths for your document loader here or make them env vars too\n",
        "    # For this example, let's assume paul_graham_essays.txt is in a 'data' subdirectory\n",
        "    doc_path = os.path.join(os.path.dirname(__file__), \"data\", \"paul_graham_essays.txt\")\n",
        "    # Create dummy data dir and file if it doesn't exist for the example to run\n",
        "    data_dir = os.path.join(os.path.dirname(__file__), \"data\")\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "    if not os.path.exists(doc_path):\n",
        "        with open(doc_path, \"w\") as f:\n",
        "            f.write(\"This is a sample essay by Paul Graham about startups and programming. It contains wisdom.\\n\")\n",
        "            f.write(\"Another paragraph discussing the importance of Lisp and hackers.\\n\")\n",
        "\n",
        "\n",
        "    retriever_cfg = RetrieverConfig(\n",
        "        loader_config=TextLoaderConfig(document_path=doc_path),\n",
        "        splitter_config=TextSplitterConfig(chunk_size=500, chunk_overlap=50), # Adjusted for example\n",
        "        embedding_config=EmbeddingConfig(), # Uses default HF_EMBED_ENDPOINT\n",
        "        vector_store_config=VectorStoreConfig(persist_directory=\"faiss_pg_essays_index\") # Example persist path\n",
        "    )\n",
        "\n",
        "    settings = AppSettings(\n",
        "        retriever_config=retriever_cfg\n",
        "        # prompt_config and llm_config will use their defaults unless overridden here\n",
        "    )\n",
        "\n",
        "    # Load actual endpoint URLs and token from environment variables\n",
        "    settings.hf_llm_endpoint_url = os.getenv(settings.llm_config.endpoint_url_env_var)\n",
        "    settings.hf_embed_endpoint_url = os.getenv(settings.retriever_config.embedding_config.endpoint_url_env_var)\n",
        "    settings.hf_api_token = os.getenv(settings.hf_token_env_var)\n",
        "\n",
        "    # Basic validation\n",
        "    if not settings.hf_llm_endpoint_url:\n",
        "        raise ValueError(f\"Environment variable {settings.llm_config.endpoint_url_env_var} not set.\")\n",
        "    if not settings.hf_embed_endpoint_url:\n",
        "        raise ValueError(f\"Environment variable {settings.retriever_config.embedding_config.endpoint_url_env_var} not set.\")\n",
        "    if not settings.hf_api_token:\n",
        "        raise ValueError(f\"Environment variable {settings.hf_token_env_var} not set.\")\n",
        "\n",
        "    print(f\"Settings loaded. LLM Endpoint: {settings.hf_llm_endpoint_url}, Embed Endpoint: {settings.hf_embed_endpoint_url}\")\n",
        "    return settings\n",
        "\n",
        "# To test loading (optional, can be removed)\n",
        "# if __name__ == \"__main__\":\n",
        "#     try:\n",
        "#         app_settings = load_settings()\n",
        "#         print(\"Successfully loaded settings:\")\n",
        "#         print(f\"  Assistant Name: {app_settings.assistant_name}\")\n",
        "#         print(f\"  Document Path: {app_settings.retriever_config.loader_config.document_path}\")\n",
        "#         print(f\"  LLM Max New Tokens: {app_settings.llm_config.max_new_tokens}\")\n",
        "#         print(f\"  HF Token Loaded: {'Yes' if app_settings.hf_api_token else 'No'}\")\n",
        "#     except ValueError as e:\n",
        "#         print(f\"Error loading settings: {e}\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"An unexpected error occurred: {e}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "qB2wfzfgivLc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}