{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donbr/session15-code-refactor/blob/main/Service_Layer_(services_py).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# services.py\n",
        "import os\n",
        "import asyncio\n",
        "from tqdm.asyncio import tqdm\n",
        "from typing import List\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEndpointEmbeddings\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableSequence\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_core.vectorstores import VectorStoreRetriever\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Assuming config_models.py is in the same directory\n",
        "from config_models import AppSettings, RetrieverConfig, LLMConfig, PromptConfig\n",
        "\n",
        "class DocumentService:\n",
        "    \"\"\"\n",
        "    Handles loading, splitting, embedding documents, and managing the vector store.\n",
        "    \"\"\"\n",
        "    def __init__(self, retriever_config: RetrieverConfig, embed_endpoint_url: str, api_token: str):\n",
        "        self.config = retriever_config\n",
        "        self.embed_endpoint_url = embed_endpoint_url\n",
        "        self.api_token = api_token\n",
        "\n",
        "        self._vectorstore: Optional[FAISS] = None\n",
        "        self._retriever: Optional[VectorStoreRetriever] = None\n",
        "\n",
        "        # Initialize embeddings\n",
        "        self.hf_embeddings = HuggingFaceEndpointEmbeddings(\n",
        "            model=self.embed_endpoint_url, # This is the HF Inference Endpoint URL\n",
        "            task=self.config.embedding_config.task,\n",
        "            huggingfacehub_api_token=self.api_token\n",
        "        )\n",
        "        print(f\"DocumentService: Initialized embeddings with endpoint: {self.embed_endpoint_url}\")\n",
        "\n",
        "    async def _load_and_split_documents(self) -> List[Document]:\n",
        "        \"\"\"Loads documents from the path specified in config and splits them.\"\"\"\n",
        "        doc_path = self.config.loader_config.document_path\n",
        "        print(f\"DocumentService: Loading documents from: {doc_path}\")\n",
        "        if not os.path.exists(doc_path):\n",
        "            raise FileNotFoundError(f\"Document file not found at {doc_path}. Please create it or update the path in config_models.py.\")\n",
        "\n",
        "        text_loader = TextLoader(doc_path)\n",
        "        # Langchain's load is synchronous, run in a thread to avoid blocking asyncio event loop\n",
        "        documents = await asyncio.to_thread(text_loader.load)\n",
        "\n",
        "        print(f\"DocumentService: Splitting {len(documents)} document(s)...\")\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=self.config.splitter_config.chunk_size,\n",
        "            chunk_overlap=self.config.splitter_config.chunk_overlap\n",
        "            # add_start_index=self.config.splitter_config.add_start_index, # If configured\n",
        "        )\n",
        "        split_documents = await asyncio.to_thread(text_splitter.split_documents, documents)\n",
        "        print(f\"DocumentService: Split into {len(split_documents)} chunks.\")\n",
        "        return split_documents\n",
        "\n",
        "    async def _index_documents_batchwise(self, split_documents: List[Document]) -> FAISS:\n",
        "        \"\"\"Indexes documents into FAISS batchwise with progress.\"\"\"\n",
        "        print(\"DocumentService: Indexing documents into FAISS...\")\n",
        "        vectorstore_instance = None\n",
        "        batches = [\n",
        "            split_documents[i:i + self.config.indexing_batch_size]\n",
        "            for i in range(0, len(split_documents), self.config.indexing_batch_size)\n",
        "        ]\n",
        "\n",
        "        first_batch_processed = False\n",
        "        # Use tqdm for progress indication\n",
        "        for i, batch in enumerate(tqdm(batches, desc=\"Indexing Batches\")):\n",
        "            if not first_batch_processed:\n",
        "                # Create the vectorstore from the first batch\n",
        "                vectorstore_instance = await FAISS.afrom_documents(batch, self.hf_embeddings)\n",
        "                first_batch_processed = True\n",
        "            else:\n",
        "                # Add subsequent batches to the existing vectorstore\n",
        "                if vectorstore_instance: # Should always be true after first batch\n",
        "                    await vectorstore_instance.aadd_documents(batch)\n",
        "            # Consider a small sleep if rate limiting is an issue, though aadd_documents handles some async internally.\n",
        "            # await asyncio.sleep(0.1)\n",
        "\n",
        "        if not vectorstore_instance:\n",
        "             raise ValueError(\"DocumentService: Failed to create vector store instance. No documents processed or an error occurred.\")\n",
        "        return vectorstore_instance\n",
        "\n",
        "\n",
        "    async def initialize_retriever(self, force_reindex: bool = False) -> VectorStoreRetriever:\n",
        "        \"\"\"\n",
        "        Initializes the vector store and retriever.\n",
        "        Tries to load from disk if persist_directory is set and index exists,\n",
        "        otherwise, re-indexes and optionally saves.\n",
        "        \"\"\"\n",
        "        persist_dir = self.config.vector_store_config.persist_directory\n",
        "\n",
        "        if not force_reindex and persist_dir and os.path.exists(persist_dir):\n",
        "            try:\n",
        "                print(f\"DocumentService: Attempting to load FAISS index from {persist_dir}\")\n",
        "                # FAISS.load_local is synchronous\n",
        "                self._vectorstore = await asyncio.to_thread(\n",
        "                    FAISS.load_local,\n",
        "                    folder_path=persist_dir,\n",
        "                    embeddings=self.hf_embeddings,\n",
        "                    allow_dangerous_deserialization=True # Required by FAISS for loading pickled data\n",
        "                )\n",
        "                print(\"DocumentService: FAISS index loaded successfully from disk.\")\n",
        "            except Exception as e:\n",
        "                print(f\"DocumentService: Failed to load FAISS index from {persist_dir}: {e}. Re-indexing.\")\n",
        "                self._vectorstore = None # Ensure it's None so re-indexing occurs\n",
        "\n",
        "        if not self._vectorstore: # If not loaded or force_reindex is True\n",
        "            print(\"DocumentService: No existing index found or re-indexing forced. Processing documents...\")\n",
        "            split_documents = await self._load_and_split_documents()\n",
        "            if not split_documents:\n",
        "                raise ValueError(\"DocumentService: No documents found or loaded to index.\")\n",
        "\n",
        "            self._vectorstore = await self._index_documents_batchwise(split_documents)\n",
        "\n",
        "            if persist_dir and self._vectorstore:\n",
        "                print(f\"DocumentService: Saving FAISS index to {persist_dir}\")\n",
        "                # FAISS.save_local is synchronous\n",
        "                await asyncio.to_thread(self._vectorstore.save_local, persist_dir)\n",
        "\n",
        "        if not self._vectorstore:\n",
        "            raise RuntimeError(\"DocumentService: Vectorstore could not be initialized.\")\n",
        "\n",
        "        # Configure retriever, e.g., with top_k if set in config\n",
        "        # search_kwargs = {}\n",
        "        # if hasattr(self.config, 'top_k_retrieval') and self.config.top_k_retrieval:\n",
        "        #     search_kwargs['k'] = self.config.top_k_retrieval\n",
        "        self._retriever = self._vectorstore.as_retriever() # search_kwargs=search_kwargs\n",
        "        print(\"DocumentService: Retriever initialized.\")\n",
        "        return self._retriever\n",
        "\n",
        "    def get_retriever(self) -> VectorStoreRetriever:\n",
        "        \"\"\"Returns the initialized retriever. Raises error if not initialized.\"\"\"\n",
        "        if not self._retriever:\n",
        "            raise ValueError(\"DocumentService: Retriever not initialized. Call initialize_retriever() first.\")\n",
        "        return self._retriever\n",
        "\n",
        "class LanguageModelService:\n",
        "    \"\"\"Handles LLM interactions using HuggingFaceEndpoint.\"\"\"\n",
        "    def __init__(self, llm_config: LLMConfig, endpoint_url: str, api_token: str):\n",
        "        self.config = llm_config\n",
        "        self.endpoint_url = endpoint_url\n",
        "        self.api_token = api_token\n",
        "\n",
        "        self.llm = HuggingFaceEndpoint(\n",
        "            endpoint_url=self.endpoint_url,\n",
        "            huggingfacehub_api_token=self.api_token,\n",
        "            task=self.config.task,\n",
        "            max_new_tokens=self.config.max_new_tokens,\n",
        "            top_k=self.config.top_k,\n",
        "            top_p=self.config.top_p,\n",
        "            typical_p=self.config.typical_p,\n",
        "            temperature=self.config.temperature,\n",
        "            repetition_penalty=self.config.repetition_penalty,\n",
        "        )\n",
        "        print(f\"LanguageModelService: Initialized LLM with endpoint: {self.endpoint_url}\")\n",
        "\n",
        "    def get_llm(self) -> HuggingFaceEndpoint:\n",
        "        return self.llm\n",
        "\n",
        "class RAGChainService:\n",
        "    \"\"\"Builds and provides the RAG LCEL chain.\"\"\"\n",
        "    def __init__(self, prompt_config: PromptConfig, llm_service: LanguageModelService):\n",
        "        self.prompt_config = prompt_config\n",
        "        self.llm_service = llm_service\n",
        "\n",
        "        self.rag_prompt_template = PromptTemplate(\n",
        "            template=self.prompt_config.template_string,\n",
        "            input_variables=self.prompt_config.input_variables\n",
        "        )\n",
        "        print(\"RAGChainService: Initialized with prompt template.\")\n",
        "\n",
        "    def build_chain(self, retriever: VectorStoreRetriever) -> RunnableSequence:\n",
        "        \"\"\"Constructs the RAG chain.\"\"\"\n",
        "\n",
        "        # Helper function to format retrieved documents\n",
        "        def format_docs(docs: List[Document]) -> str:\n",
        "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "        # Define the RAG chain using LCEL\n",
        "        # The input to this chain will be a dictionary like {\"query\": \"user's question\"}\n",
        "        rag_chain = (\n",
        "            {\n",
        "                \"context\": retriever | format_docs,  # Retrieve context then format it\n",
        "                \"query\": RunnablePassthrough()  # Pass the original query through\n",
        "            }\n",
        "            | self.rag_prompt_template          # Populate the prompt template\n",
        "            | self.llm_service.get_llm()       # Send to the LLM\n",
        "            | StrOutputParser()                # Parse the LLM output as a string\n",
        "        )\n",
        "        print(\"RAGChainService: RAG chain built.\")\n",
        "        return rag_chain"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "pNv8JPvmik8u"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}